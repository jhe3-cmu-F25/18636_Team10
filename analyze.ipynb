{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Reading Excel file: Top500Website.xlsx\n",
      "ğŸŒ Loaded 400 domains (processing first 400).\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 1/400: www.google.com\n",
      "============================\n",
      "â­ï¸ Skipping www.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 2/400: www.blogger.com\n",
      "============================\n",
      "â­ï¸ Skipping www.blogger.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 3/400: youtube.com\n",
      "============================\n",
      "â­ï¸ Skipping youtube.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 4/400: linkedin.com\n",
      "============================\n",
      "â­ï¸ Skipping linkedin.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 5/400: support.google.com\n",
      "============================\n",
      "â­ï¸ Skipping support.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 6/400: cloudflare.com\n",
      "============================\n",
      "â­ï¸ Skipping cloudflare.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 7/400: microsoft.com\n",
      "============================\n",
      "â­ï¸ Skipping microsoft.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 8/400: apple.com\n",
      "============================\n",
      "â­ï¸ Skipping apple.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 9/400: en.wikipedia.org\n",
      "============================\n",
      "â­ï¸ Skipping en.wikipedia.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 10/400: play.google.com\n",
      "============================\n",
      "â­ï¸ Skipping play.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 11/400: wordpress.org\n",
      "============================\n",
      "â­ï¸ Skipping wordpress.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 12/400: docs.google.com\n",
      "============================\n",
      "â­ï¸ Skipping docs.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 13/400: mozilla.org\n",
      "============================\n",
      "â­ï¸ Skipping mozilla.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 14/400: maps.google.com\n",
      "============================\n",
      "â­ï¸ Skipping maps.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 15/400: youtu.be\n",
      "============================\n",
      "â­ï¸ Skipping youtu.be (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 16/400: drive.google.com\n",
      "============================\n",
      "â­ï¸ Skipping drive.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 17/400: bp.blogspot.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://bp.blogspot.com\n",
      "Application Started: [ 11/30/2025, 10:34:25 PM ]\n",
      "\n",
      "[INFO] Crawling https://bp.blogspot.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://bp.blogspot.com:  Error: net::ERR_NAME_NOT_RESOLVED at https://bp.blogspot.com\n",
      "    at navigate \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:155:23\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
      "    at async FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:130:21\u001b[90m)\u001b[39m\n",
      "    at async Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:16\u001b[90m)\u001b[39m\n",
      "    at async Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:16\u001b[90m)\u001b[39m\n",
      "    at async Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:5\u001b[90m)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m\n",
      "    at async main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping bp.blogspot.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 18/400: sites.google.com\n",
      "============================\n",
      "â­ï¸ Skipping sites.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 19/400: googleusercontent.com\n",
      "============================\n",
      "â­ï¸ Skipping googleusercontent.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 20/400: accounts.google.com\n",
      "============================\n",
      "â­ï¸ Skipping accounts.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 21/400: t.me\n",
      "============================\n",
      "â­ï¸ Skipping t.me (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 22/400: europa.eu\n",
      "============================\n",
      "â­ï¸ Skipping europa.eu (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 23/400: plus.google.com\n",
      "============================\n",
      "â­ï¸ Skipping plus.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 24/400: whatsapp.com\n",
      "============================\n",
      "â­ï¸ Skipping whatsapp.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 25/400: adobe.com\n",
      "============================\n",
      "â­ï¸ Skipping adobe.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 26/400: facebook.com\n",
      "============================\n",
      "â­ï¸ Skipping facebook.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 27/400: policies.google.com\n",
      "============================\n",
      "â­ï¸ Skipping policies.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 28/400: uol.com.br\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://uol.com.br\n",
      "Application Started: [ 11/30/2025, 10:34:33 PM ]\n",
      "\n",
      "[INFO] Crawling https://uol.com.br\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://uol.com.br:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping uol.com.br due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 29/400: istockphoto.com\n",
      "============================\n",
      "â­ï¸ Skipping istockphoto.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 30/400: vimeo.com\n",
      "============================\n",
      "â­ï¸ Skipping vimeo.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 31/400: vk.com\n",
      "============================\n",
      "â­ï¸ Skipping vk.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 32/400: github.com\n",
      "============================\n",
      "â­ï¸ Skipping github.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 33/400: amazon.com\n",
      "============================\n",
      "â­ï¸ Skipping amazon.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 34/400: search.google.com\n",
      "============================\n",
      "â­ï¸ Skipping search.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 35/400: bbc.co.uk\n",
      "============================\n",
      "â­ï¸ Skipping bbc.co.uk (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 36/400: google.de\n",
      "============================\n",
      "â­ï¸ Skipping google.de (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 37/400: live.com\n",
      "============================\n",
      "â­ï¸ Skipping live.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 38/400: gravatar.com\n",
      "============================\n",
      "â­ï¸ Skipping gravatar.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 39/400: nih.gov\n",
      "============================\n",
      "â­ï¸ Skipping nih.gov (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 40/400: dan.com\n",
      "============================\n",
      "â­ï¸ Skipping dan.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 41/400: files.wordpress.com\n",
      "============================\n",
      "â­ï¸ Skipping files.wordpress.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 42/400: www.yahoo.com\n",
      "============================\n",
      "â­ï¸ Skipping www.yahoo.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 43/400: cnn.com\n",
      "============================\n",
      "â­ï¸ Skipping cnn.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 44/400: dropbox.com\n",
      "============================\n",
      "â­ï¸ Skipping dropbox.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 45/400: wikimedia.org\n",
      "============================\n",
      "â­ï¸ Skipping wikimedia.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 46/400: creativecommons.org\n",
      "============================\n",
      "â­ï¸ Skipping creativecommons.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 47/400: google.com.br\n",
      "============================\n",
      "â­ï¸ Skipping google.com.br (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 48/400: line.me\n",
      "============================\n",
      "â­ï¸ Skipping line.me (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 49/400: googleblog.com\n",
      "============================\n",
      "â­ï¸ Skipping googleblog.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 50/400: opera.com\n",
      "============================\n",
      "â­ï¸ Skipping opera.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 51/400: es.wikipedia.org\n",
      "============================\n",
      "â­ï¸ Skipping es.wikipedia.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 52/400: globo.com\n",
      "============================\n",
      "â­ï¸ Skipping globo.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 53/400: brandbucket.com\n",
      "============================\n",
      "â­ï¸ Skipping brandbucket.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 54/400: myspace.com\n",
      "============================\n",
      "â­ï¸ Skipping myspace.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 55/400: slideshare.net\n",
      "============================\n",
      "â­ï¸ Skipping slideshare.net (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 56/400: paypal.com\n",
      "============================\n",
      "â­ï¸ Skipping paypal.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 57/400: tiktok.com\n",
      "============================\n",
      "â­ï¸ Skipping tiktok.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 58/400: netvibes.com\n",
      "============================\n",
      "â­ï¸ Skipping netvibes.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 59/400: theguardian.com\n",
      "============================\n",
      "â­ï¸ Skipping theguardian.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 60/400: who.int\n",
      "============================\n",
      "â­ï¸ Skipping who.int (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 61/400: goo.gl\n",
      "============================\n",
      "â­ï¸ Skipping goo.gl (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 62/400: medium.com\n",
      "============================\n",
      "â­ï¸ Skipping medium.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 63/400: tools.google.com\n",
      "============================\n",
      "â­ï¸ Skipping tools.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 64/400: draft.blogger.com\n",
      "============================\n",
      "â­ï¸ Skipping draft.blogger.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 65/400: pt.wikipedia.org\n",
      "============================\n",
      "â­ï¸ Skipping pt.wikipedia.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 66/400: fr.wikipedia.org\n",
      "============================\n",
      "â­ï¸ Skipping fr.wikipedia.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 67/400: www.weebly.com\n",
      "============================\n",
      "â­ï¸ Skipping www.weebly.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 68/400: news.google.com\n",
      "============================\n",
      "â­ï¸ Skipping news.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 69/400: developers.google.com\n",
      "============================\n",
      "â­ï¸ Skipping developers.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 70/400: w3.org\n",
      "============================\n",
      "â­ï¸ Skipping w3.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 71/400: mail.google.com\n",
      "============================\n",
      "â­ï¸ Skipping mail.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 72/400: gstatic.com\n",
      "============================\n",
      "â­ï¸ Skipping gstatic.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 73/400: jimdofree.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://jimdofree.com\n",
      "Application Started: [ 11/30/2025, 10:35:18 PM ]\n",
      "\n",
      "[INFO] Crawling https://jimdofree.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://jimdofree.com:  ProtocolError: Protocol error (Page.navigate): Target closed.\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:226:24\n",
      "    at new Promise (<anonymous>)\n",
      "    at CDPSession.send \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:222:16\u001b[90m)\u001b[39m\n",
      "    at navigate \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:148:47\u001b[90m)\u001b[39m\n",
      "    at FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:131:13\u001b[90m)\u001b[39m\n",
      "    at Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:41\u001b[90m)\u001b[39m\n",
      "    at Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:53\u001b[90m)\u001b[39m\n",
      "    at Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:16\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m {\n",
      "  originalMessage: \u001b[90mundefined\u001b[39m\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping jimdofree.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 74/400: cpanel.net\n",
      "============================\n",
      "â­ï¸ Skipping cpanel.net (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 75/400: imdb.com\n",
      "============================\n",
      "â­ï¸ Skipping imdb.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 76/400: wa.me\n",
      "============================\n",
      "â­ï¸ Skipping wa.me (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 77/400: feedburner.com\n",
      "============================\n",
      "â­ï¸ Skipping feedburner.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 78/400: enable-javascript.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://enable-javascript.com\n",
      "Application Started: [ 11/30/2025, 10:35:23 PM ]\n",
      "\n",
      "[INFO] Crawling https://enable-javascript.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/enable-javascript_com\n",
      "Application Started: [ 11/30/2025, 10:35:34 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/enable-javascript_com\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/enable-javascript_com: Error: ENOENT: no such file or directory, open '../Results/enable-javascript_com/cookies.json'\n",
      "\u001b[90m    at Object.readFileSync (node:fs:441:20)\u001b[39m\n",
      "    at LoadData \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mhelpers/storage.js:13:21\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:21:30\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m {\n",
      "  errno: \u001b[33m-2\u001b[39m,\n",
      "  code: \u001b[32m'ENOENT'\u001b[39m,\n",
      "  syscall: \u001b[32m'open'\u001b[39m,\n",
      "  path: \u001b[32m'../Results/enable-javascript_com/cookies.json'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Detector attempt 2/3 for ../Results/enable-javascript_com\n",
      "Application Started: [ 11/30/2025, 10:35:36 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/enable-javascript_com\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/enable-javascript_com: Error: ENOENT: no such file or directory, open '../Results/enable-javascript_com/cookies.json'\n",
      "\u001b[90m    at Object.readFileSync (node:fs:441:20)\u001b[39m\n",
      "    at LoadData \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mhelpers/storage.js:13:21\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:21:30\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m {\n",
      "  errno: \u001b[33m-2\u001b[39m,\n",
      "  code: \u001b[32m'ENOENT'\u001b[39m,\n",
      "  syscall: \u001b[32m'open'\u001b[39m,\n",
      "  path: \u001b[32m'../Results/enable-javascript_com/cookies.json'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Detector attempt 3/3 for ../Results/enable-javascript_com\n",
      "Application Started: [ 11/30/2025, 10:35:39 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/enable-javascript_com\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/enable-javascript_com: Error: ENOENT: no such file or directory, open '../Results/enable-javascript_com/cookies.json'\n",
      "\u001b[90m    at Object.readFileSync (node:fs:441:20)\u001b[39m\n",
      "    at LoadData \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mhelpers/storage.js:13:21\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:21:30\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m {\n",
      "  errno: \u001b[33m-2\u001b[39m,\n",
      "  code: \u001b[32m'ENOENT'\u001b[39m,\n",
      "  syscall: \u001b[32m'open'\u001b[39m,\n",
      "  path: \u001b[32m'../Results/enable-javascript_com/cookies.json'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›” Detector failed after 3 attempts\n",
      "ğŸš« Skipping enable-javascript.com due to Detector failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 79/400: nytimes.com\n",
      "============================\n",
      "â­ï¸ Skipping nytimes.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 80/400: workspace.google.com\n",
      "============================\n",
      "â­ï¸ Skipping workspace.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 81/400: ok.ru\n",
      "============================\n",
      "â­ï¸ Skipping ok.ru (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 82/400: google.es\n",
      "============================\n",
      "â­ï¸ Skipping google.es (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 83/400: dailymotion.com\n",
      "============================\n",
      "â­ï¸ Skipping dailymotion.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 84/400: afternic.com\n",
      "============================\n",
      "â­ï¸ Skipping afternic.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 85/400: bloomberg.com\n",
      "============================\n",
      "â­ï¸ Skipping bloomberg.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 86/400: amazon.de\n",
      "============================\n",
      "â­ï¸ Skipping amazon.de (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 87/400: photos.google.com\n",
      "============================\n",
      "â­ï¸ Skipping photos.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 88/400: wiley.com\n",
      "============================\n",
      "â­ï¸ Skipping wiley.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 89/400: aliexpress.com\n",
      "============================\n",
      "â­ï¸ Skipping aliexpress.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 90/400: indiatimes.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://indiatimes.com\n",
      "Application Started: [ 11/30/2025, 10:35:41 PM ]\n",
      "\n",
      "[INFO] Crawling https://indiatimes.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://indiatimes.com:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping indiatimes.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 91/400: youronlinechoices.com\n",
      "============================\n",
      "â­ï¸ Skipping youronlinechoices.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 92/400: elpais.com\n",
      "============================\n",
      "â­ï¸ Skipping elpais.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 93/400: tinyurl.com\n",
      "============================\n",
      "â­ï¸ Skipping tinyurl.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 94/400: yadi.sk\n",
      "============================\n",
      "â­ï¸ Skipping yadi.sk (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 95/400: spotify.com\n",
      "============================\n",
      "â­ï¸ Skipping spotify.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 96/400: huffpost.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://huffpost.com\n",
      "Application Started: [ 11/30/2025, 10:36:27 PM ]\n",
      "\n",
      "[INFO] Crawling https://huffpost.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://huffpost.com:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping huffpost.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 97/400: ru.wikipedia.org\n",
      "============================\n",
      "â­ï¸ Skipping ru.wikipedia.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 98/400: google.fr\n",
      "============================\n",
      "â­ï¸ Skipping google.fr (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 99/400: webmd.com\n",
      "============================\n",
      "â­ï¸ Skipping webmd.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 100/400: samsung.com\n",
      "============================\n",
      "â­ï¸ Skipping samsung.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 101/400: independent.co.uk\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://independent.co.uk\n",
      "Application Started: [ 11/30/2025, 10:37:13 PM ]\n",
      "\n",
      "[INFO] Crawling https://independent.co.uk\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://independent.co.uk:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping independent.co.uk due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 102/400: amazon.co.jp\n",
      "============================\n",
      "â­ï¸ Skipping amazon.co.jp (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 103/400: get.google.com\n",
      "============================\n",
      "â­ï¸ Skipping get.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 104/400: amazon.co.uk\n",
      "============================\n",
      "â­ï¸ Skipping amazon.co.uk (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 105/400: 4shared.com\n",
      "============================\n",
      "â­ï¸ Skipping 4shared.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 106/400: telegram.me\n",
      "============================\n",
      "â­ï¸ Skipping telegram.me (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 107/400: planalto.gov.br\n",
      "============================\n",
      "â­ï¸ Skipping planalto.gov.br (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 108/400: businessinsider.com\n",
      "============================\n",
      "â­ï¸ Skipping businessinsider.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 109/400: ig.com.br\n",
      "============================\n",
      "â­ï¸ Skipping ig.com.br (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 110/400: issuu.com\n",
      "============================\n",
      "â­ï¸ Skipping issuu.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 111/400: www.gov.br\n",
      "============================\n",
      "â­ï¸ Skipping www.gov.br (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 112/400: wsj.com\n",
      "============================\n",
      "â­ï¸ Skipping wsj.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 113/400: hugedomains.com\n",
      "============================\n",
      "â­ï¸ Skipping hugedomains.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 114/400: picasaweb.google.com\n",
      "============================\n",
      "â­ï¸ Skipping picasaweb.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 115/400: usatoday.com\n",
      "============================\n",
      "â­ï¸ Skipping usatoday.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 116/400: scribd.com\n",
      "============================\n",
      "â­ï¸ Skipping scribd.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 117/400: www.gov.uk\n",
      "============================\n",
      "â­ï¸ Skipping www.gov.uk (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 118/400: storage.googleapis.com\n",
      "============================\n",
      "â­ï¸ Skipping storage.googleapis.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 119/400: huffingtonpost.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://huffingtonpost.com\n",
      "Application Started: [ 11/30/2025, 10:37:58 PM ]\n",
      "\n",
      "[INFO] Crawling https://huffingtonpost.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://huffingtonpost.com:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping huffingtonpost.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 120/400: bbc.com\n",
      "============================\n",
      "â­ï¸ Skipping bbc.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 121/400: estadao.com.br\n",
      "============================\n",
      "â­ï¸ Skipping estadao.com.br (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 122/400: nature.com\n",
      "============================\n",
      "â­ï¸ Skipping nature.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 123/400: mediafire.com\n",
      "============================\n",
      "â­ï¸ Skipping mediafire.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 124/400: washingtonpost.com\n",
      "============================\n",
      "â­ï¸ Skipping washingtonpost.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 125/400: forms.gle\n",
      "============================\n",
      "â­ï¸ Skipping forms.gle (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 126/400: namecheap.com\n",
      "============================\n",
      "â­ï¸ Skipping namecheap.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 127/400: forbes.com\n",
      "============================\n",
      "â­ï¸ Skipping forbes.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 128/400: mirror.co.uk\n",
      "============================\n",
      "â­ï¸ Skipping mirror.co.uk (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 129/400: soundcloud.com\n",
      "============================\n",
      "â­ï¸ Skipping soundcloud.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 130/400: fb.com\n",
      "============================\n",
      "â­ï¸ Skipping fb.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 131/400: marketingplatform.google....\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://marketingplatform.google....\n",
      "Application Started: [ 11/30/2025, 10:38:45 PM ]\n",
      "\n",
      "[INFO] Crawling https://marketingplatform.google....\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://marketingplatform.google....:  Error: net::ERR_NAME_NOT_RESOLVED at https://marketingplatform.google....\n",
      "    at navigate \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:155:23\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
      "    at async FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:130:21\u001b[90m)\u001b[39m\n",
      "    at async Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:16\u001b[90m)\u001b[39m\n",
      "    at async Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:16\u001b[90m)\u001b[39m\n",
      "    at async Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:5\u001b[90m)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m\n",
      "    at async main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping marketingplatform.google.... due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 132/400: domainmarket.com\n",
      "============================\n",
      "â­ï¸ Skipping domainmarket.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 133/400: ytimg.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://ytimg.com\n",
      "Application Started: [ 11/30/2025, 10:38:50 PM ]\n",
      "\n",
      "[INFO] Crawling https://ytimg.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://ytimg.com:  Error: net::ERR_NAME_NOT_RESOLVED at https://ytimg.com\n",
      "    at navigate \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:155:23\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
      "    at async FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:130:21\u001b[90m)\u001b[39m\n",
      "    at async Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:16\u001b[90m)\u001b[39m\n",
      "    at async Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:16\u001b[90m)\u001b[39m\n",
      "    at async Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:5\u001b[90m)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m\n",
      "    at async main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping ytimg.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 134/400: terra.com.br\n",
      "============================\n",
      "â­ï¸ Skipping terra.com.br (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 135/400: google.co.uk\n",
      "============================\n",
      "â­ï¸ Skipping google.co.uk (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 136/400: shutterstock.com\n",
      "============================\n",
      "â­ï¸ Skipping shutterstock.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 137/400: dailymail.co.uk\n",
      "============================\n",
      "â­ï¸ Skipping dailymail.co.uk (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 138/400: reg.ru\n",
      "============================\n",
      "â­ï¸ Skipping reg.ru (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 139/400: t.co\n",
      "============================\n",
      "â­ï¸ Skipping t.co (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 140/400: cdc.gov\n",
      "============================\n",
      "â­ï¸ Skipping cdc.gov (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 141/400: thesun.co.uk\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://thesun.co.uk\n",
      "Application Started: [ 11/30/2025, 10:38:53 PM ]\n",
      "\n",
      "[INFO] Crawling https://thesun.co.uk\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/thesun_co_uk\n",
      "Application Started: [ 11/30/2025, 10:39:13 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/thesun_co_uk\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m8\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m84\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m314\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/thesun_co_uk/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 142/400: wp.com\n",
      "============================\n",
      "â­ï¸ Skipping wp.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 143/400: cnet.com\n",
      "============================\n",
      "â­ï¸ Skipping cnet.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 144/400: instagram.com\n",
      "============================\n",
      "â­ï¸ Skipping instagram.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 145/400: researchgate.net\n",
      "============================\n",
      "â­ï¸ Skipping researchgate.net (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 146/400: google.it\n",
      "============================\n",
      "â­ï¸ Skipping google.it (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 147/400: fandom.com\n",
      "============================\n",
      "â­ï¸ Skipping fandom.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 148/400: office.com\n",
      "============================\n",
      "â­ï¸ Skipping office.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 149/400: list-manage.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://list-manage.com\n",
      "Application Started: [ 11/30/2025, 10:39:17 PM ]\n",
      "\n",
      "[INFO] Crawling https://list-manage.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://list-manage.com:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n",
      "    at async FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:130:21\u001b[90m)\u001b[39m\n",
      "    at async Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:16\u001b[90m)\u001b[39m\n",
      "    at async Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:16\u001b[90m)\u001b[39m\n",
      "    at async Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:5\u001b[90m)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m\n",
      "    at async main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping list-manage.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 150/400: msn.com\n",
      "============================\n",
      "â­ï¸ Skipping msn.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 151/400: un.org\n",
      "============================\n",
      "â­ï¸ Skipping un.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 152/400: de.wikipedia.org\n",
      "============================\n",
      "â­ï¸ Skipping de.wikipedia.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 153/400: ovh.com\n",
      "============================\n",
      "â­ï¸ Skipping ovh.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 154/400: mail.ru\n",
      "============================\n",
      "â­ï¸ Skipping mail.ru (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 155/400: bing.com\n",
      "============================\n",
      "â­ï¸ Skipping bing.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 156/400: news.yahoo.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://news.yahoo.com\n",
      "Application Started: [ 11/30/2025, 10:40:02 PM ]\n",
      "\n",
      "[INFO] Crawling https://news.yahoo.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://news.yahoo.com:  Error: Navigation failed because browser has disconnected!\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:51:147\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/vendor/mitt/src/index.js:51:62\n",
      "    at Array.map (<anonymous>)\n",
      "    at Object.emit \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/vendor/mitt/src/index.js:51:43\u001b[90m)\u001b[39m\n",
      "    at CDPSession.emit \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/EventEmitter.js:72:22\u001b[90m)\u001b[39m\n",
      "    at CDPSession._onClosed \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:267:14\u001b[90m)\u001b[39m\n",
      "    at Connection._onClose \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:144:21\u001b[90m)\u001b[39m\n",
      "    at WebSocket.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/node/NodeWebSocketTransport.js:17:30\u001b[90m)\u001b[39m\n",
      "    at WebSocket.onClose \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mws\u001b[24m/lib/event-target.js:210:18\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at WebSocket.emit (node:events:519:28)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping news.yahoo.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 157/400: myaccount.google.com\n",
      "============================\n",
      "â­ï¸ Skipping myaccount.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 158/400: hatena.ne.jp\n",
      "============================\n",
      "â­ï¸ Skipping hatena.ne.jp (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 159/400: shopify.com\n",
      "============================\n",
      "â­ï¸ Skipping shopify.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 160/400: adssettings.google.com\n",
      "============================\n",
      "â­ï¸ Skipping adssettings.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 161/400: bit.ly\n",
      "============================\n",
      "â­ï¸ Skipping bit.ly (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 162/400: reuters.com\n",
      "============================\n",
      "â­ï¸ Skipping reuters.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 163/400: booking.com\n",
      "============================\n",
      "â­ï¸ Skipping booking.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 164/400: discord.com\n",
      "============================\n",
      "â­ï¸ Skipping discord.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 165/400: buydomains.com\n",
      "============================\n",
      "â­ï¸ Skipping buydomains.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 166/400: nasa.gov\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://nasa.gov\n",
      "Application Started: [ 11/30/2025, 10:40:17 PM ]\n",
      "\n",
      "[INFO] Crawling https://nasa.gov\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/nasa_gov\n",
      "Application Started: [ 11/30/2025, 10:40:28 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/nasa_gov\n",
      "[INFO] Loaded data\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/nasa_gov: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(Cookies.IsCookieValid(cookie))\n",
      "\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:206:9\n",
      "    at Array.forEach (<anonymous>)\n",
      "    at CookieLeak \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:205:13\u001b[90m)\u001b[39m\n",
      "    at IdLeaking \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:228:12\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:37:25\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33m0\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Detector attempt 2/3 for ../Results/nasa_gov\n",
      "Application Started: [ 11/30/2025, 10:40:31 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/nasa_gov\n",
      "[INFO] Loaded data\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/nasa_gov: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(Cookies.IsCookieValid(cookie))\n",
      "\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:206:9\n",
      "    at Array.forEach (<anonymous>)\n",
      "    at CookieLeak \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:205:13\u001b[90m)\u001b[39m\n",
      "    at IdLeaking \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:228:12\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:37:25\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33m0\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Detector attempt 3/3 for ../Results/nasa_gov\n",
      "Application Started: [ 11/30/2025, 10:40:33 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/nasa_gov\n",
      "[INFO] Loaded data\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/nasa_gov: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(Cookies.IsCookieValid(cookie))\n",
      "\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:206:9\n",
      "    at Array.forEach (<anonymous>)\n",
      "    at CookieLeak \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:205:13\u001b[90m)\u001b[39m\n",
      "    at IdLeaking \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:228:12\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:37:25\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33m0\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›” Detector failed after 3 attempts\n",
      "ğŸš« Skipping nasa.gov due to Detector failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 167/400: aboutads.info\n",
      "============================\n",
      "â­ï¸ Skipping aboutads.info (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 168/400: time.com\n",
      "============================\n",
      "â­ï¸ Skipping time.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 169/400: abril.com.br\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://abril.com.br\n",
      "Application Started: [ 11/30/2025, 10:40:35 PM ]\n",
      "\n",
      "[INFO] Crawling https://abril.com.br\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/abril_com_br\n",
      "Application Started: [ 11/30/2025, 10:40:48 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/abril_com_br\n",
      "[INFO] Loaded data\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/abril_com_br: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(Cookies.IsCookieValid(cookie))\n",
      "\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:206:9\n",
      "    at Array.forEach (<anonymous>)\n",
      "    at CookieLeak \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:205:13\u001b[90m)\u001b[39m\n",
      "    at IdLeaking \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:228:12\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:37:25\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33m0\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Detector attempt 2/3 for ../Results/abril_com_br\n",
      "Application Started: [ 11/30/2025, 10:40:50 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/abril_com_br\n",
      "[INFO] Loaded data\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/abril_com_br: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(Cookies.IsCookieValid(cookie))\n",
      "\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:206:9\n",
      "    at Array.forEach (<anonymous>)\n",
      "    at CookieLeak \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:205:13\u001b[90m)\u001b[39m\n",
      "    at IdLeaking \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:228:12\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:37:25\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33m0\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Detector attempt 3/3 for ../Results/abril_com_br\n",
      "Application Started: [ 11/30/2025, 10:40:52 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/abril_com_br\n",
      "[INFO] Loaded data\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/abril_com_br: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(Cookies.IsCookieValid(cookie))\n",
      "\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:206:9\n",
      "    at Array.forEach (<anonymous>)\n",
      "    at CookieLeak \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:205:13\u001b[90m)\u001b[39m\n",
      "    at IdLeaking \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mdetectors/leaks.js:228:12\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:37:25\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33m0\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›” Detector failed after 3 attempts\n",
      "ğŸš« Skipping abril.com.br due to Detector failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 170/400: change.org\n",
      "============================\n",
      "â­ï¸ Skipping change.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 171/400: nginx.org\n",
      "============================\n",
      "â­ï¸ Skipping nginx.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 172/400: twitter.com\n",
      "============================\n",
      "â­ï¸ Skipping twitter.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 173/400: www.wikipedia.org\n",
      "============================\n",
      "â­ï¸ Skipping www.wikipedia.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 174/400: archive.org\n",
      "============================\n",
      "â­ï¸ Skipping archive.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 175/400: cbsnews.com\n",
      "============================\n",
      "â­ï¸ Skipping cbsnews.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 176/400: networkadvertising.org\n",
      "============================\n",
      "â­ï¸ Skipping networkadvertising.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 177/400: telegraph.co.uk\n",
      "============================\n",
      "â­ï¸ Skipping telegraph.co.uk (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 178/400: pinterest.com\n",
      "============================\n",
      "â­ï¸ Skipping pinterest.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 179/400: google.co.jp\n",
      "============================\n",
      "â­ï¸ Skipping google.co.jp (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 180/400: pixabay.com\n",
      "============================\n",
      "â­ï¸ Skipping pixabay.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 181/400: zendesk.com\n",
      "============================\n",
      "â­ï¸ Skipping zendesk.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 182/400: cpanel.com\n",
      "============================\n",
      "â­ï¸ Skipping cpanel.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 183/400: vistaprint.com\n",
      "============================\n",
      "â­ï¸ Skipping vistaprint.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 184/400: sky.com\n",
      "============================\n",
      "â­ï¸ Skipping sky.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 185/400: windows.net\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://windows.net\n",
      "Application Started: [ 11/30/2025, 10:40:55 PM ]\n",
      "\n",
      "[INFO] Crawling https://windows.net\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://windows.net:  Error: net::ERR_CERT_COMMON_NAME_INVALID at https://windows.net\n",
      "    at navigate \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:155:23\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
      "    at async FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:130:21\u001b[90m)\u001b[39m\n",
      "    at async Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:16\u001b[90m)\u001b[39m\n",
      "    at async Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:16\u001b[90m)\u001b[39m\n",
      "    at async Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:5\u001b[90m)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m\n",
      "    at async main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping windows.net due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 186/400: alicdn.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://alicdn.com\n",
      "Application Started: [ 11/30/2025, 10:40:58 PM ]\n",
      "\n",
      "[INFO] Crawling https://alicdn.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://alicdn.com:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n",
      "    at async FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:130:21\u001b[90m)\u001b[39m\n",
      "    at async Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:16\u001b[90m)\u001b[39m\n",
      "    at async Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:16\u001b[90m)\u001b[39m\n",
      "    at async Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:5\u001b[90m)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m\n",
      "    at async main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping alicdn.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 187/400: google.ca\n",
      "============================\n",
      "â­ï¸ Skipping google.ca (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 188/400: lemonde.fr\n",
      "============================\n",
      "â­ï¸ Skipping lemonde.fr (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 189/400: newyorker.com\n",
      "============================\n",
      "â­ï¸ Skipping newyorker.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 190/400: webnode.page\n",
      "============================\n",
      "â­ï¸ Skipping webnode.page (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 191/400: surveymonkey.com\n",
      "============================\n",
      "â­ï¸ Skipping surveymonkey.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 192/400: translate.google.com\n",
      "============================\n",
      "â­ï¸ Skipping translate.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 193/400: calendar.google.com\n",
      "============================\n",
      "â­ï¸ Skipping calendar.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 194/400: amazonaws.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://amazonaws.com\n",
      "Application Started: [ 11/30/2025, 10:41:41 PM ]\n",
      "\n",
      "[INFO] Crawling https://amazonaws.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://amazonaws.com:  Error: net::ERR_CONNECTION_REFUSED at https://amazonaws.com\n",
      "    at navigate \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:155:23\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
      "    at async FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:130:21\u001b[90m)\u001b[39m\n",
      "    at async Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:16\u001b[90m)\u001b[39m\n",
      "    at async Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:16\u001b[90m)\u001b[39m\n",
      "    at async Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:5\u001b[90m)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m\n",
      "    at async main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping amazonaws.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 195/400: academia.edu\n",
      "============================\n",
      "â­ï¸ Skipping academia.edu (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 196/400: apache.org\n",
      "============================\n",
      "â­ï¸ Skipping apache.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 197/400: imageshack.us\n",
      "============================\n",
      "â­ï¸ Skipping imageshack.us (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 198/400: akamaihd.net\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://akamaihd.net\n",
      "Application Started: [ 11/30/2025, 10:41:48 PM ]\n",
      "\n",
      "[INFO] Crawling https://akamaihd.net\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://akamaihd.net:  Error: net::ERR_NAME_NOT_RESOLVED at https://akamaihd.net\n",
      "    at navigate \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:155:23\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
      "    at async FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:130:21\u001b[90m)\u001b[39m\n",
      "    at async Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:16\u001b[90m)\u001b[39m\n",
      "    at async Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:16\u001b[90m)\u001b[39m\n",
      "    at async Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:5\u001b[90m)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m\n",
      "    at async main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping akamaihd.net due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 199/400: nginx.com\n",
      "============================\n",
      "â­ï¸ Skipping nginx.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 200/400: discord.gg\n",
      "============================\n",
      "â­ï¸ Skipping discord.gg (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 201/400: thetimes.co.uk\n",
      "============================\n",
      "â­ï¸ Skipping thetimes.co.uk (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 202/400: search.yahoo.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://search.yahoo.com\n",
      "Application Started: [ 11/30/2025, 10:41:51 PM ]\n",
      "\n",
      "[INFO] Crawling https://search.yahoo.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/search_yahoo_com\n",
      "Application Started: [ 11/30/2025, 10:42:01 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/search_yahoo_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m4\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/search_yahoo_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 203/400: amazon.fr\n",
      "============================\n",
      "â­ï¸ Skipping amazon.fr (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 204/400: yelp.com\n",
      "============================\n",
      "â­ï¸ Skipping yelp.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 205/400: berkeley.edu\n",
      "============================\n",
      "â­ï¸ Skipping berkeley.edu (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 206/400: google.ru\n",
      "============================\n",
      "â­ï¸ Skipping google.ru (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 207/400: sedoparking.com\n",
      "============================\n",
      "â­ï¸ Skipping sedoparking.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 208/400: cbc.ca\n",
      "============================\n",
      "â­ï¸ Skipping cbc.ca (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 209/400: unesco.org\n",
      "============================\n",
      "â­ï¸ Skipping unesco.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 210/400: ggpht.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://ggpht.com\n",
      "Application Started: [ 11/30/2025, 10:42:02 PM ]\n",
      "\n",
      "[INFO] Crawling https://ggpht.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://ggpht.com:  Error: net::ERR_NAME_NOT_RESOLVED at https://ggpht.com\n",
      "    at navigate \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:155:23\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
      "    at async FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:130:21\u001b[90m)\u001b[39m\n",
      "    at async Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:16\u001b[90m)\u001b[39m\n",
      "    at async Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:16\u001b[90m)\u001b[39m\n",
      "    at async Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:5\u001b[90m)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m\n",
      "    at async main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping ggpht.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 211/400: privacyshield.gov\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://privacyshield.gov\n",
      "Application Started: [ 11/30/2025, 10:42:05 PM ]\n",
      "\n",
      "[INFO] Crawling https://privacyshield.gov\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://privacyshield.gov:  Error: net::ERR_CONNECTION_RESET at https://privacyshield.gov\n",
      "    at navigate \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:155:23\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
      "    at async FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:130:21\u001b[90m)\u001b[39m\n",
      "    at async Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:16\u001b[90m)\u001b[39m\n",
      "    at async Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:16\u001b[90m)\u001b[39m\n",
      "    at async Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:5\u001b[90m)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m\n",
      "    at async main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping privacyshield.gov due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 212/400: www.over-blog.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://www.over-blog.com\n",
      "Application Started: [ 11/30/2025, 10:42:09 PM ]\n",
      "\n",
      "[INFO] Crawling https://www.over-blog.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://www.over-blog.com:  Error: Protocol error (Network.getCookies): Session closed. Most likely the page has been closed.\n",
      "    at CDPSession.send \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:214:35\u001b[90m)\u001b[39m\n",
      "    at Page.cookies \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:733:53\u001b[90m)\u001b[39m\n",
      "    at Object.GetFirstPartyCookies \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mscrapers/cookies.js:26:23\u001b[90m)\u001b[39m\n",
      "    at Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:45:46\u001b[90m)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m\n",
      "    at async main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping www.over-blog.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 213/400: clarin.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://clarin.com\n",
      "Application Started: [ 11/30/2025, 10:42:21 PM ]\n",
      "\n",
      "[INFO] Crawling https://clarin.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://clarin.com:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping clarin.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 214/400: www.wix.com\n",
      "============================\n",
      "â­ï¸ Skipping www.wix.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 215/400: whitehouse.gov\n",
      "============================\n",
      "â­ï¸ Skipping whitehouse.gov (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 216/400: icann.org\n",
      "============================\n",
      "â­ï¸ Skipping icann.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 217/400: gnu.org\n",
      "============================\n",
      "â­ï¸ Skipping gnu.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 218/400: yandex.ru\n",
      "============================\n",
      "â­ï¸ Skipping yandex.ru (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 219/400: francetvinfo.fr\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://francetvinfo.fr\n",
      "Application Started: [ 11/30/2025, 10:43:05 PM ]\n",
      "\n",
      "[INFO] Crawling https://francetvinfo.fr\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://francetvinfo.fr:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping francetvinfo.fr due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 220/400: gmail.com\n",
      "============================\n",
      "â­ï¸ Skipping gmail.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 221/400: mozilla.com\n",
      "============================\n",
      "â­ï¸ Skipping mozilla.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 222/400: ziddu.com\n",
      "============================\n",
      "â­ï¸ Skipping ziddu.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 223/400: guardian.co.uk\n",
      "============================\n",
      "â­ï¸ Skipping guardian.co.uk (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 224/400: twitch.tv\n",
      "============================\n",
      "â­ï¸ Skipping twitch.tv (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 225/400: sedo.com\n",
      "============================\n",
      "â­ï¸ Skipping sedo.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 226/400: foxnews.com\n",
      "============================\n",
      "â­ï¸ Skipping foxnews.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 227/400: rambler.ru\n",
      "============================\n",
      "â­ï¸ Skipping rambler.ru (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 228/400: books.google.com\n",
      "============================\n",
      "â­ï¸ Skipping books.google.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 229/400: stanford.edu\n",
      "============================\n",
      "â­ï¸ Skipping stanford.edu (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 230/400: wikihow.com\n",
      "============================\n",
      "â­ï¸ Skipping wikihow.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 231/400: it.wikipedia.org\n",
      "============================\n",
      "â­ï¸ Skipping it.wikipedia.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 232/400: 20minutos.es\n",
      "============================\n",
      "â­ï¸ Skipping 20minutos.es (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 233/400: sfgate.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://sfgate.com\n",
      "Application Started: [ 11/30/2025, 10:43:51 PM ]\n",
      "\n",
      "[INFO] Crawling https://sfgate.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://sfgate.com:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping sfgate.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 234/400: liveinternet.ru\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://liveinternet.ru\n",
      "Application Started: [ 11/30/2025, 10:44:36 PM ]\n",
      "\n",
      "[INFO] Crawling https://liveinternet.ru\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/liveinternet_ru\n",
      "Application Started: [ 11/30/2025, 10:44:58 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/liveinternet_ru\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m108\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m235\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/liveinternet_ru/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 235/400: ja.wikipedia.org\n",
      "============================\n",
      "â­ï¸ Skipping ja.wikipedia.org (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 236/400: 000webhost.com\n",
      "============================\n",
      "â­ï¸ Skipping 000webhost.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 237/400: espn.com\n",
      "============================\n",
      "â­ï¸ Skipping espn.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 238/400: eventbrite.com\n",
      "============================\n",
      "â­ï¸ Skipping eventbrite.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 239/400: disney.com\n",
      "============================\n",
      "â­ï¸ Skipping disney.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 240/400: statista.com\n",
      "============================\n",
      "â­ï¸ Skipping statista.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 241/400: addthis.com\n",
      "============================\n",
      "â­ï¸ Skipping addthis.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 242/400: pinterest.fr\n",
      "============================\n",
      "â­ï¸ Skipping pinterest.fr (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 243/400: lavanguardia.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://lavanguardia.com\n",
      "Application Started: [ 11/30/2025, 10:45:02 PM ]\n",
      "\n",
      "[INFO] Crawling https://lavanguardia.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/lavanguardia_com\n",
      "Application Started: [ 11/30/2025, 10:45:16 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/lavanguardia_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m4\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m1\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m81\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/lavanguardia_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 244/400: vkontakte.ru\n",
      "============================\n",
      "â­ï¸ Skipping vkontakte.ru (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 245/400: doubleclick.net\n",
      "============================\n",
      "â­ï¸ Skipping doubleclick.net (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 246/400: bp2.blogger.com\n",
      "============================\n",
      "â­ï¸ Skipping bp2.blogger.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 247/400: skype.com\n",
      "============================\n",
      "â­ï¸ Skipping skype.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 248/400: sciencedaily.com\n",
      "============================\n",
      "â­ï¸ Skipping sciencedaily.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 249/400: bloglovin.com\n",
      "============================\n",
      "â­ï¸ Skipping bloglovin.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 250/400: insider.com\n",
      "============================\n",
      "â­ï¸ Skipping insider.com (result.json already exists)\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 251/400: pl.wikipedia.org\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://pl.wikipedia.org\n",
      "Application Started: [ 11/30/2025, 10:45:18 PM ]\n",
      "\n",
      "[INFO] Crawling https://pl.wikipedia.org\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/pl_wikipedia_org\n",
      "Application Started: [ 11/30/2025, 10:45:27 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/pl_wikipedia_org\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m2\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/pl_wikipedia_org/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 252/400: sputniknews.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://sputniknews.com\n",
      "Application Started: [ 11/30/2025, 10:45:28 PM ]\n",
      "\n",
      "[INFO] Crawling https://sputniknews.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/sputniknews_com\n",
      "Application Started: [ 11/30/2025, 10:45:43 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/sputniknews_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m2\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m13\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/sputniknews_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 253/400: id.wikipedia.org\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://id.wikipedia.org\n",
      "Application Started: [ 11/30/2025, 10:45:44 PM ]\n",
      "\n",
      "[INFO] Crawling https://id.wikipedia.org\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/id_wikipedia_org\n",
      "Application Started: [ 11/30/2025, 10:45:53 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/id_wikipedia_org\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m3\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/id_wikipedia_org/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 254/400: doi.org\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://doi.org\n",
      "Application Started: [ 11/30/2025, 10:45:55 PM ]\n",
      "\n",
      "[INFO] Crawling https://doi.org\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/doi_org\n",
      "Application Started: [ 11/30/2025, 10:46:03 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/doi_org\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m7\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/doi_org/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 255/400: nypost.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://nypost.com\n",
      "Application Started: [ 11/30/2025, 10:46:05 PM ]\n",
      "\n",
      "[INFO] Crawling https://nypost.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://nypost.com:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping nypost.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 256/400: elmundo.es\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://elmundo.es\n",
      "Application Started: [ 11/30/2025, 10:46:51 PM ]\n",
      "\n",
      "[INFO] Crawling https://elmundo.es\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/elmundo_es\n",
      "Application Started: [ 11/30/2025, 10:47:10 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/elmundo_es\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m11\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m80\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/elmundo_es/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 257/400: abcnews.go.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://abcnews.go.com\n",
      "Application Started: [ 11/30/2025, 10:47:11 PM ]\n",
      "\n",
      "[INFO] Crawling https://abcnews.go.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/abcnews_go_com\n",
      "Application Started: [ 11/30/2025, 10:47:28 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/abcnews_go_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m4\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m5\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m70\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/abcnews_go_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 258/400: ipv4.google.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://ipv4.google.com\n",
      "Application Started: [ 11/30/2025, 10:47:29 PM ]\n",
      "\n",
      "[INFO] Crawling https://ipv4.google.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://ipv4.google.com:  Error: Navigation failed because browser has disconnected!\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:51:147\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/vendor/mitt/src/index.js:51:62\n",
      "    at Array.map (<anonymous>)\n",
      "    at Object.emit \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/vendor/mitt/src/index.js:51:43\u001b[90m)\u001b[39m\n",
      "    at CDPSession.emit \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/EventEmitter.js:72:22\u001b[90m)\u001b[39m\n",
      "    at CDPSession._onClosed \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:267:14\u001b[90m)\u001b[39m\n",
      "    at Connection._onClose \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:144:21\u001b[90m)\u001b[39m\n",
      "    at WebSocket.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/node/NodeWebSocketTransport.js:17:30\u001b[90m)\u001b[39m\n",
      "    at WebSocket.onClose \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mws\u001b[24m/lib/event-target.js:210:18\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at WebSocket.emit (node:events:519:28)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping ipv4.google.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 259/400: deezer.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://deezer.com\n",
      "Application Started: [ 11/30/2025, 10:47:35 PM ]\n",
      "\n",
      "[INFO] Crawling https://deezer.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/deezer_com\n",
      "Application Started: [ 11/30/2025, 10:47:47 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/deezer_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m7\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/deezer_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 260/400: express.co.uk\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://express.co.uk\n",
      "Application Started: [ 11/30/2025, 10:47:48 PM ]\n",
      "\n",
      "[INFO] Crawling https://express.co.uk\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/express_co_uk\n",
      "Application Started: [ 11/30/2025, 10:47:59 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/express_co_uk\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m2\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m22\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/express_co_uk/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 261/400: detik.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://detik.com\n",
      "Application Started: [ 11/30/2025, 10:48:00 PM ]\n",
      "\n",
      "[INFO] Crawling https://detik.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/detik_com\n",
      "Application Started: [ 11/30/2025, 10:48:18 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/detik_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m5\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m39\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m159\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/detik_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 262/400: mystrikingly.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://mystrikingly.com\n",
      "Application Started: [ 11/30/2025, 10:48:20 PM ]\n",
      "\n",
      "[INFO] Crawling https://mystrikingly.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/mystrikingly_com\n",
      "Application Started: [ 11/30/2025, 10:48:34 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/mystrikingly_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m2\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m20\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/mystrikingly_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 263/400: rakuten.co.jp\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://rakuten.co.jp\n",
      "Application Started: [ 11/30/2025, 10:48:35 PM ]\n",
      "\n",
      "[INFO] Crawling https://rakuten.co.jp\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/rakuten_co_jp\n",
      "Application Started: [ 11/30/2025, 10:48:50 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/rakuten_co_jp\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m10\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m19\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m112\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/rakuten_co_jp/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 264/400: amzn.to\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://amzn.to\n",
      "Application Started: [ 11/30/2025, 10:48:52 PM ]\n",
      "\n",
      "[INFO] Crawling https://amzn.to\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/amzn_to\n",
      "Application Started: [ 11/30/2025, 10:49:05 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/amzn_to\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m1\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m14\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m62\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/amzn_to/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 265/400: arxiv.org\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://arxiv.org\n",
      "Application Started: [ 11/30/2025, 10:49:06 PM ]\n",
      "\n",
      "[INFO] Crawling https://arxiv.org\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/arxiv_org\n",
      "Application Started: [ 11/30/2025, 10:49:17 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/arxiv_org\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m1\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/arxiv_org/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 266/400: alibaba.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://alibaba.com\n",
      "Application Started: [ 11/30/2025, 10:49:18 PM ]\n",
      "\n",
      "[INFO] Crawling https://alibaba.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/alibaba_com\n",
      "Application Started: [ 11/30/2025, 10:49:30 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/alibaba_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m4\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m13\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m72\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/alibaba_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 267/400: fb.me\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://fb.me\n",
      "Application Started: [ 11/30/2025, 10:49:31 PM ]\n",
      "\n",
      "[INFO] Crawling https://fb.me\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/fb_me\n",
      "Application Started: [ 11/30/2025, 10:49:41 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/fb_me\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m2\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/fb_me/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 268/400: wikia.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://wikia.com\n",
      "Application Started: [ 11/30/2025, 10:49:42 PM ]\n",
      "\n",
      "[INFO] Crawling https://wikia.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/wikia_com\n",
      "Application Started: [ 11/30/2025, 10:49:51 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/wikia_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m3\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m28\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/wikia_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 269/400: t-online.de\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://t-online.de\n",
      "Application Started: [ 11/30/2025, 10:49:52 PM ]\n",
      "\n",
      "[INFO] Crawling https://t-online.de\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/t-online_de\n",
      "Application Started: [ 11/30/2025, 10:50:05 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/t-online_de\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/t-online_de: Error: ENOENT: no such file or directory, open '../Results/t-online_de/cookies.json'\n",
      "\u001b[90m    at Object.readFileSync (node:fs:441:20)\u001b[39m\n",
      "    at LoadData \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mhelpers/storage.js:13:21\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:21:30\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m {\n",
      "  errno: \u001b[33m-2\u001b[39m,\n",
      "  code: \u001b[32m'ENOENT'\u001b[39m,\n",
      "  syscall: \u001b[32m'open'\u001b[39m,\n",
      "  path: \u001b[32m'../Results/t-online_de/cookies.json'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Detector attempt 2/3 for ../Results/t-online_de\n",
      "Application Started: [ 11/30/2025, 10:50:07 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/t-online_de\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/t-online_de: Error: ENOENT: no such file or directory, open '../Results/t-online_de/cookies.json'\n",
      "\u001b[90m    at Object.readFileSync (node:fs:441:20)\u001b[39m\n",
      "    at LoadData \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mhelpers/storage.js:13:21\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:21:30\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m {\n",
      "  errno: \u001b[33m-2\u001b[39m,\n",
      "  code: \u001b[32m'ENOENT'\u001b[39m,\n",
      "  syscall: \u001b[32m'open'\u001b[39m,\n",
      "  path: \u001b[32m'../Results/t-online_de/cookies.json'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Detector attempt 3/3 for ../Results/t-online_de\n",
      "Application Started: [ 11/30/2025, 10:50:09 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/t-online_de\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/t-online_de: Error: ENOENT: no such file or directory, open '../Results/t-online_de/cookies.json'\n",
      "\u001b[90m    at Object.readFileSync (node:fs:441:20)\u001b[39m\n",
      "    at LoadData \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mhelpers/storage.js:13:21\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:21:30\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m {\n",
      "  errno: \u001b[33m-2\u001b[39m,\n",
      "  code: \u001b[32m'ENOENT'\u001b[39m,\n",
      "  syscall: \u001b[32m'open'\u001b[39m,\n",
      "  path: \u001b[32m'../Results/t-online_de/cookies.json'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›” Detector failed after 3 attempts\n",
      "ğŸš« Skipping t-online.de due to Detector failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 270/400: telegra.ph\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://telegra.ph\n",
      "Application Started: [ 11/30/2025, 10:50:11 PM ]\n",
      "\n",
      "[INFO] Crawling https://telegra.ph\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/telegra_ph\n",
      "Application Started: [ 11/30/2025, 10:50:20 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/telegra_ph\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m3\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/telegra_ph/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 271/400: mega.nz\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://mega.nz\n",
      "Application Started: [ 11/30/2025, 10:50:22 PM ]\n",
      "\n",
      "[INFO] Crawling https://mega.nz\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/mega_nz\n",
      "Application Started: [ 11/30/2025, 10:50:35 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/mega_nz\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m6\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/mega_nz/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 272/400: usnews.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://usnews.com\n",
      "Application Started: [ 11/30/2025, 10:50:36 PM ]\n",
      "\n",
      "[INFO] Crawling https://usnews.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://usnews.com:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping usnews.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 273/400: plos.org\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://plos.org\n",
      "Application Started: [ 11/30/2025, 10:51:20 PM ]\n",
      "\n",
      "[INFO] Crawling https://plos.org\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/plos_org\n",
      "Application Started: [ 11/30/2025, 10:51:35 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/plos_org\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m3\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m21\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/plos_org/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 274/400: naver.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://naver.com\n",
      "Application Started: [ 11/30/2025, 10:51:36 PM ]\n",
      "\n",
      "[INFO] Crawling https://naver.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/naver_com\n",
      "Application Started: [ 11/30/2025, 10:51:47 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/naver_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m18\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/naver_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 275/400: ibm.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://ibm.com\n",
      "Application Started: [ 11/30/2025, 10:51:48 PM ]\n",
      "\n",
      "[INFO] Crawling https://ibm.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/ibm_com\n",
      "Application Started: [ 11/30/2025, 10:52:03 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/ibm_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m25\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m19\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m99\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/ibm_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 276/400: smh.com.au\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://smh.com.au\n",
      "Application Started: [ 11/30/2025, 10:52:05 PM ]\n",
      "\n",
      "[INFO] Crawling https://smh.com.au\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/smh_com_au\n",
      "Application Started: [ 11/30/2025, 10:52:24 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/smh_com_au\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m2\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m30\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m161\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/smh_com_au/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 277/400: dw.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://dw.com\n",
      "Application Started: [ 11/30/2025, 10:52:26 PM ]\n",
      "\n",
      "[INFO] Crawling https://dw.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/dw_com\n",
      "Application Started: [ 11/30/2025, 10:52:38 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/dw_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m3\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m6\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/dw_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 278/400: google.nl\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://google.nl\n",
      "Application Started: [ 11/30/2025, 10:52:39 PM ]\n",
      "\n",
      "[INFO] Crawling https://google.nl\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/google_nl\n",
      "Application Started: [ 11/30/2025, 10:52:48 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/google_nl\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m8\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/google_nl/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 279/400: lefigaro.fr\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://lefigaro.fr\n",
      "Application Started: [ 11/30/2025, 10:52:50 PM ]\n",
      "\n",
      "[INFO] Crawling https://lefigaro.fr\n",
      "[INFO] Created browser instance\n",
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://lefigaro.fr:  ProtocolError: Protocol error (Target.attachToTarget): Target closed.\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:75:24\n",
      "    at new Promise (<anonymous>)\n",
      "    at Connection.send \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:71:16\u001b[90m)\u001b[39m\n",
      "    at Connection.createSession \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:157:42\u001b[90m)\u001b[39m\n",
      "    at Target._sessionFactory \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Browser.js:192:91\u001b[90m)\u001b[39m\n",
      "    at Target.page \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Target.js:73:38\u001b[90m)\u001b[39m\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Browser.js:463:37\n",
      "    at Array.map (<anonymous>)\n",
      "    at BrowserContext.pages \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Browser.js:463:14\u001b[90m)\u001b[39m\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Browser.js:330:96 {\n",
      "  originalMessage: \u001b[90mundefined\u001b[39m\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping lefigaro.fr due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 280/400: bp1.blogger.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://bp1.blogger.com\n",
      "Application Started: [ 11/30/2025, 10:52:52 PM ]\n",
      "\n",
      "[INFO] Crawling https://bp1.blogger.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/bp1_blogger_com\n",
      "Application Started: [ 11/30/2025, 10:53:02 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/bp1_blogger_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m2\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/bp1_blogger_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 281/400: picasa.google.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://picasa.google.com\n",
      "Application Started: [ 11/30/2025, 10:53:03 PM ]\n",
      "\n",
      "[INFO] Crawling https://picasa.google.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/picasa_google_com\n",
      "Application Started: [ 11/30/2025, 10:53:12 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/picasa_google_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m6\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/picasa_google_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 282/400: theatlantic.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://theatlantic.com\n",
      "Application Started: [ 11/30/2025, 10:53:13 PM ]\n",
      "\n",
      "[INFO] Crawling https://theatlantic.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/theatlantic_com\n",
      "Application Started: [ 11/30/2025, 10:53:26 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/theatlantic_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m9\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m33\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m154\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/theatlantic_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 283/400: nydailynews.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://nydailynews.com\n",
      "Application Started: [ 11/30/2025, 10:53:28 PM ]\n",
      "\n",
      "[INFO] Crawling https://nydailynews.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://nydailynews.com:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping nydailynews.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 284/400: themeforest.net\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://themeforest.net\n",
      "Application Started: [ 11/30/2025, 10:54:14 PM ]\n",
      "\n",
      "[INFO] Crawling https://themeforest.net\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/themeforest_net\n",
      "Application Started: [ 11/30/2025, 10:54:25 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/themeforest_net\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m5\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/themeforest_net/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 285/400: rtve.es\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://rtve.es\n",
      "Application Started: [ 11/30/2025, 10:54:27 PM ]\n",
      "\n",
      "[INFO] Crawling https://rtve.es\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://rtve.es:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping rtve.es due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 286/400: newsweek.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://newsweek.com\n",
      "Application Started: [ 11/30/2025, 10:55:10 PM ]\n",
      "\n",
      "[INFO] Crawling https://newsweek.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/newsweek_com\n",
      "Application Started: [ 11/30/2025, 10:55:22 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/newsweek_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m15\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m26\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m201\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/newsweek_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 287/400: ovh.net\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://ovh.net\n",
      "Application Started: [ 11/30/2025, 10:55:24 PM ]\n",
      "\n",
      "[INFO] Crawling https://ovh.net\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/ovh_net\n",
      "Application Started: [ 11/30/2025, 10:55:37 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/ovh_net\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m12\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/ovh_net/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 288/400: ca.gov\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://ca.gov\n",
      "Application Started: [ 11/30/2025, 10:55:38 PM ]\n",
      "\n",
      "[INFO] Crawling https://ca.gov\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://ca.gov:  ProtocolError: Protocol error (Page.navigate): Target closed.\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:226:24\n",
      "    at new Promise (<anonymous>)\n",
      "    at CDPSession.send \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Connection.js:222:16\u001b[90m)\u001b[39m\n",
      "    at navigate \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:148:47\u001b[90m)\u001b[39m\n",
      "    at FrameManager.navigateFrame \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:131:13\u001b[90m)\u001b[39m\n",
      "    at Frame.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/FrameManager.js:500:41\u001b[90m)\u001b[39m\n",
      "    at Page.goto \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/Page.js:1167:53\u001b[90m)\u001b[39m\n",
      "    at Scrape \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mdriver/scrape.js:40:16\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
      "    at async CollectTraces \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mapp.js:32:24\u001b[90m)\u001b[39m {\n",
      "  originalMessage: \u001b[90mundefined\u001b[39m\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping ca.gov due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 289/400: goodreads.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://goodreads.com\n",
      "Application Started: [ 11/30/2025, 10:55:42 PM ]\n",
      "\n",
      "[INFO] Crawling https://goodreads.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/goodreads_com\n",
      "Application Started: [ 11/30/2025, 10:55:54 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/goodreads_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m1\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m1\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m29\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/goodreads_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 290/400: economist.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://economist.com\n",
      "Application Started: [ 11/30/2025, 10:55:55 PM ]\n",
      "\n",
      "[INFO] Crawling https://economist.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/economist_com\n",
      "Application Started: [ 11/30/2025, 10:56:15 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/economist_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m12\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m7\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m96\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/economist_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 291/400: target.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://target.com\n",
      "Application Started: [ 11/30/2025, 10:56:16 PM ]\n",
      "\n",
      "[INFO] Crawling https://target.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/target_com\n",
      "Application Started: [ 11/30/2025, 10:56:36 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/target_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m8\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m29\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/target_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 292/400: marca.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://marca.com\n",
      "Application Started: [ 11/30/2025, 10:56:38 PM ]\n",
      "\n",
      "[INFO] Crawling https://marca.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/marca_com\n",
      "Application Started: [ 11/30/2025, 10:56:51 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/marca_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m5\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m35\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/marca_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 293/400: kickstarter.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://kickstarter.com\n",
      "Application Started: [ 11/30/2025, 10:56:52 PM ]\n",
      "\n",
      "[INFO] Crawling https://kickstarter.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/kickstarter_com\n",
      "Application Started: [ 11/30/2025, 10:57:08 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/kickstarter_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m4\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m26\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/kickstarter_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 294/400: hindustantimes.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://hindustantimes.com\n",
      "Application Started: [ 11/30/2025, 10:57:09 PM ]\n",
      "\n",
      "[INFO] Crawling https://hindustantimes.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error at https://hindustantimes.com:  TimeoutError: Navigation timeout of 40000 ms exceeded\n",
      "    at \u001b[90m/Users/qie/Desktop/consent-guard-main/Source/Crawler/\u001b[39mnode_modules/\u001b[4mpuppeteer\u001b[24m/lib/cjs/puppeteer/common/LifecycleWatcher.js:106:111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] No traces collected\n",
      "âŒ Crawler failed, retrying...\n",
      "â›” Crawler failed after 3 attempts\n",
      "ğŸš« Skipping hindustantimes.com due to Crawler failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 295/400: weibo.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://weibo.com\n",
      "Application Started: [ 11/30/2025, 10:57:54 PM ]\n",
      "\n",
      "[INFO] Crawling https://weibo.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/weibo_com\n",
      "Application Started: [ 11/30/2025, 10:58:06 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/weibo_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m0\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m3\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/weibo_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 296/400: finance.yahoo.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://finance.yahoo.com\n",
      "Application Started: [ 11/30/2025, 10:58:08 PM ]\n",
      "\n",
      "[INFO] Crawling https://finance.yahoo.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/finance_yahoo_com\n",
      "Application Started: [ 11/30/2025, 10:58:21 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/finance_yahoo_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m3\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m49\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m158\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/finance_yahoo_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 297/400: huawei.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://huawei.com\n",
      "Application Started: [ 11/30/2025, 10:58:23 PM ]\n",
      "\n",
      "[INFO] Crawling https://huawei.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/huawei_com\n",
      "Application Started: [ 11/30/2025, 10:58:36 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/huawei_com\n",
      "[INFO] Loaded data\n",
      "[INFO] Detected \u001b[33m3\u001b[39m first-party ID leaking violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m third-party ID synchronization violations\n",
      "[INFO] Detected \u001b[33m0\u001b[39m browser fingerprinting functions\n",
      "[INFO] Found network traffic towards \u001b[33m7\u001b[39m third-parties\n",
      "[INFO] Processed website\n",
      "[INFO] Stored data to ../Results/huawei_com/result.json\n",
      "âœ… Detector success\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 298/400: e-monsite.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://e-monsite.com\n",
      "Application Started: [ 11/30/2025, 10:58:37 PM ]\n",
      "\n",
      "[INFO] Crawling https://e-monsite.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n",
      "[INFO] Waiting...\n",
      "[INFO] Collecting data...\n",
      "[INFO] Storing data...\n",
      "[INFO] Process complete\n",
      "âœ… Crawler success\n",
      "\n",
      "ğŸ” Detector attempt 1/3 for ../Results/e-monsite_com\n",
      "Application Started: [ 11/30/2025, 10:58:57 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/e-monsite_com\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/e-monsite_com: Error: ENOENT: no such file or directory, open '../Results/e-monsite_com/cookies.json'\n",
      "\u001b[90m    at Object.readFileSync (node:fs:441:20)\u001b[39m\n",
      "    at LoadData \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mhelpers/storage.js:13:21\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:21:30\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m {\n",
      "  errno: \u001b[33m-2\u001b[39m,\n",
      "  code: \u001b[32m'ENOENT'\u001b[39m,\n",
      "  syscall: \u001b[32m'open'\u001b[39m,\n",
      "  path: \u001b[32m'../Results/e-monsite_com/cookies.json'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Detector attempt 2/3 for ../Results/e-monsite_com\n",
      "Application Started: [ 11/30/2025, 10:58:59 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/e-monsite_com\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/e-monsite_com: Error: ENOENT: no such file or directory, open '../Results/e-monsite_com/cookies.json'\n",
      "\u001b[90m    at Object.readFileSync (node:fs:441:20)\u001b[39m\n",
      "    at LoadData \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mhelpers/storage.js:13:21\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:21:30\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m {\n",
      "  errno: \u001b[33m-2\u001b[39m,\n",
      "  code: \u001b[32m'ENOENT'\u001b[39m,\n",
      "  syscall: \u001b[32m'open'\u001b[39m,\n",
      "  path: \u001b[32m'../Results/e-monsite_com/cookies.json'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Detector attempt 3/3 for ../Results/e-monsite_com\n",
      "Application Started: [ 11/30/2025, 10:59:01 PM ]\n",
      "\n",
      "[INFO] Processing ../Results/e-monsite_com\n",
      "âŒ Detector failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation failed for ../Results/e-monsite_com: Error: ENOENT: no such file or directory, open '../Results/e-monsite_com/cookies.json'\n",
      "\u001b[90m    at Object.readFileSync (node:fs:441:20)\u001b[39m\n",
      "    at LoadData \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mhelpers/storage.js:13:21\u001b[90m)\u001b[39m\n",
      "    at Analyse \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:21:30\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:83:24\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m {\n",
      "  errno: \u001b[33m-2\u001b[39m,\n",
      "  code: \u001b[32m'ENOENT'\u001b[39m,\n",
      "  syscall: \u001b[32m'open'\u001b[39m,\n",
      "  path: \u001b[32m'../Results/e-monsite_com/cookies.json'\u001b[39m\n",
      "}\n",
      "Fatal error: AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:\n",
      "\n",
      "  assert(typeof(violations.idLeaking) == \"object\")\n",
      "\n",
      "    at PrintViolationsInfo \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:67:5\u001b[90m)\u001b[39m\n",
      "    at ProcessWebsite \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:85:5\u001b[90m)\u001b[39m\n",
      "    at main \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:121:28\u001b[90m)\u001b[39m\n",
      "    at Object.<anonymous> \u001b[90m(/Users/qie/Desktop/consent-guard-main/Source/Detector/\u001b[39mapp.js:142:3\u001b[90m)\u001b[39m\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1706:14)\u001b[39m\n",
      "\u001b[90m    at Object..js (node:internal/modules/cjs/loader:1839:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1441:32)\u001b[39m\n",
      "\u001b[90m    at Function._load (node:internal/modules/cjs/loader:1263:12)\u001b[39m\n",
      "\u001b[90m    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\u001b[39m\n",
      "\u001b[90m    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\u001b[39m {\n",
      "  generatedMessage: \u001b[33mtrue\u001b[39m,\n",
      "  code: \u001b[32m'ERR_ASSERTION'\u001b[39m,\n",
      "  actual: \u001b[33mfalse\u001b[39m,\n",
      "  expected: \u001b[33mtrue\u001b[39m,\n",
      "  operator: \u001b[32m'=='\u001b[39m,\n",
      "  diff: \u001b[32m'simple'\u001b[39m\n",
      "}\n",
      "Exiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›” Detector failed after 3 attempts\n",
      "ğŸš« Skipping e-monsite.com due to Detector failure\n",
      "\n",
      "============================\n",
      "ğŸš€ Processing website 299/400: hubspot.com\n",
      "============================\n",
      "\n",
      "ğŸ” Crawler attempt 1/1 for https://hubspot.com\n",
      "Application Started: [ 11/30/2025, 10:59:03 PM ]\n",
      "\n",
      "[INFO] Crawling https://hubspot.com\n",
      "[INFO] Created browser instance\n",
      "[INFO] Visiting website...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 216\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ‰ ALL DONE!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 216\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 175\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# --- Step 1: Crawler with Retry ---\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mrun_crawler_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš« Skipping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m due to Crawler failure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m, in \u001b[0;36mrun_crawler_with_retry\u001b[0;34m(url, retries)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ” Crawler attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapp.js\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCRAWLER_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Crawler success\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:507\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    509\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:1126\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1124\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:1189\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1917\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:1875\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1875\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1877\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1879\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1880\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "# ============= CONFIG ============= #\n",
    "XLSX_PATH = \"Top500Website.xlsx\"     \n",
    "ROOT_DOMAIN_COLUMN = \"Root Domain\"  \n",
    "CRAWLER_PATH = \"Source/Crawler\"\n",
    "DETECTOR_PATH = \"Source/Detector\"\n",
    "RESULTS_PATH = \"Source/Results\"\n",
    "MAX_WEBSITES = 400  \n",
    "# ================================== #\n",
    "\n",
    "\n",
    "def normalize_domain(domain: str) -> str:\n",
    "    domain = domain.strip()\n",
    "    if domain.startswith(\"http://\") or domain.startswith(\"https://\"):\n",
    "        return domain\n",
    "    return \"https://\" + domain\n",
    "\n",
    "\n",
    "def domain_to_folder(domain: str) -> str:\n",
    "    \"\"\"Convert domain like 'www.bbc.com' into folder name: www_bbc_com.\"\"\"\n",
    "    return (\n",
    "        domain.replace(\"https://\", \"\")\n",
    "            .replace(\"http://\", \"\")\n",
    "            .replace(\".\", \"_\")\n",
    "    )\n",
    "\n",
    "\n",
    "# Retry Mechanism for Crawler & Detector \n",
    "def run_crawler_with_retry(url, retries=1):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        print(f\"\\nğŸ” Crawler attempt {attempt}/{retries} for {url}\")\n",
    "        result = subprocess.run([\"node\", \"app.js\", url], cwd=CRAWLER_PATH)\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Crawler success\")\n",
    "            return True\n",
    "        print(\"âŒ Crawler failed, retrying...\")\n",
    "        time.sleep(2)\n",
    "    print(\"â›” Crawler failed after 3 attempts\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def run_detector_with_retry(folder_path, retries=3):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        print(f\"\\nğŸ” Detector attempt {attempt}/{retries} for {folder_path}\")\n",
    "        result = subprocess.run([\"node\", \"app.js\", folder_path], cwd=DETECTOR_PATH)\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Detector success\")\n",
    "            return True\n",
    "        print(\"âŒ Detector failed, retrying...\")\n",
    "        time.sleep(2)\n",
    "    print(\"â›” Detector failed after 3 attempts\")\n",
    "    return False\n",
    "\n",
    "\n",
    "# Inline Analyzer\n",
    "def parse_results_inlined(results_dir=\"Source/Results\"):\n",
    "    data = []\n",
    "    website_dirs = [\n",
    "        d for d in os.listdir(results_dir)\n",
    "        if os.path.isdir(os.path.join(results_dir, d))\n",
    "    ]\n",
    "    for site_dir in website_dirs:\n",
    "        result_path = os.path.join(results_dir, site_dir, \"result.json\")\n",
    "        if not os.path.exists(result_path):\n",
    "            print(f\"âš ï¸ result.json NOT found for {site_dir}, skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            with open(result_path, \"r\") as f:\n",
    "                result = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"âš ï¸ JSON decode error for {site_dir}, skipping.\")\n",
    "            continue\n",
    "        # Extract violation counts\n",
    "        id_leaking_count = len(result.get(\"idLeaking\", []))\n",
    "        cookie_sync_count = len(result.get(\"cookieSync\", []))\n",
    "        fingerprinting_obj = result.get(\"fingerprinting\", {})\n",
    "        fingerprinting_count = len(fingerprinting_obj.get(\"functions\", []))\n",
    "        total_violations = (\n",
    "                id_leaking_count +\n",
    "                cookie_sync_count +\n",
    "                fingerprinting_count\n",
    "        )\n",
    "        is_compliant = (total_violations == 0)\n",
    "        website_name = site_dir.replace(\"www_\", \"\").replace(\"_\", \".\")\n",
    "        data.append({\n",
    "            \"Website\": website_name,\n",
    "            \"ID_Leaking_Count\": id_leaking_count,\n",
    "            \"Cookie_Sync_Count\": cookie_sync_count,\n",
    "            \"Fingerprinting_Count\": fingerprinting_count,\n",
    "            \"Total_Violations\": total_violations,\n",
    "            \"Is_Compliant\": is_compliant\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸ“„ Reading Excel file:\", XLSX_PATH)\n",
    "    df = pd.read_excel(XLSX_PATH)\n",
    "    if ROOT_DOMAIN_COLUMN not in df.columns:\n",
    "        print(f\"âŒ ERROR: Column '{ROOT_DOMAIN_COLUMN}' not found in Excel!\")\n",
    "        return\n",
    "    domains = df[ROOT_DOMAIN_COLUMN].dropna().astype(str).tolist()\n",
    "    domains = domains[:MAX_WEBSITES]\n",
    "    print(f\"ğŸŒ Loaded {len(domains)} domains (processing first {MAX_WEBSITES}).\")\n",
    "    os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "    # Process each website\n",
    "    for i, domain in enumerate(domains, 1):\n",
    "        print(\"\\n============================\")\n",
    "        print(f\"ğŸš€ Processing website {i}/{MAX_WEBSITES}: {domain}\")\n",
    "        print(\"============================\")\n",
    "        url = normalize_domain(domain)\n",
    "        folder_name = domain_to_folder(domain)\n",
    "        website_result_dir = os.path.join(RESULTS_PATH, folder_name)\n",
    "        os.makedirs(website_result_dir, exist_ok=True)\n",
    "        result_json_path = os.path.join(website_result_dir, \"result.json\")\n",
    "        if os.path.exists(result_json_path):\n",
    "            print(f\"â­ï¸ Skipping {domain} (result.json already exists)\")\n",
    "            continue\n",
    "        # --- Step 1: Crawler with Retry ---\n",
    "        if not run_crawler_with_retry(url):\n",
    "            print(f\"ğŸš« Skipping {domain} due to Crawler failure\")\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "        # --- Step 2: Detector with Retry ---\n",
    "        detector_path = \"../Results/\" + folder_name\n",
    "        if not run_detector_with_retry(detector_path):\n",
    "            print(f\"ğŸš« Skipping {domain} due to Detector failure\")\n",
    "            continue\n",
    "        time.sleep(1)\n",
    "    # Final Analysis\n",
    "    print(\"\\nğŸ“Š Running final analysisâ€¦\\n\")\n",
    "    df_result = parse_results_inlined(RESULTS_PATH)\n",
    "    if not df_result.empty:\n",
    "        output_path = \"Result.xlsx\"\n",
    "        df_result.to_excel(output_path, index=False)\n",
    "        print(f\"âœ… Result saved to: {output_path}\")\n",
    "        print(\"\\n--- Compliance Summary ---\")\n",
    "        print(df_result)\n",
    "        compliant = df_result[\"Is_Compliant\"].sum()\n",
    "        total = len(df_result)\n",
    "        non_compliant = total - compliant\n",
    "        print(f\"\\nTotal Sites: {total}\")\n",
    "        print(f\"Compliant: {compliant}\")\n",
    "        print(f\"Non-Compliant: {non_compliant}\")\n",
    "        print(f\"Non-Compliance Rate: {non_compliant / total * 100:.2f}%\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No valid result.json files found.\")\n",
    "    print(\"\\nğŸ‰ ALL DONE!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
